{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 框架的使用和示例：线性回归数据\n",
    "\n",
    "此笔记本是一个线性回归问题的例子，用来帮助用户理解和掌握如何使用训练器框架。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from core.trainer import Trainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 训练器实例\n",
    "\n",
    "训练器 `core.trainer.Trainer` 实例化时，需指定以下参数：\n",
    "- exp_name: 实验名称\n",
    "- epoch: 本次训练轮数\n",
    "- batch_size: 批数据数量\n",
    "- gradient_accumulation_step(=1): 梯度累积步数\n",
    "- init_random_seed(=None): 初始随机种子\n",
    "- device(=\"cpu\"): 训练设备\n",
    "- enable_auto_mixed_precision(=True): 自动混合精度\n",
    "- log_tool(=\"tensorboard\"): 日志工具\n",
    "- log_dir(=None): 日志目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    exp_name=\"lin-reg\",\n",
    "    epoch=20,\n",
    "    batch_size=40,\n",
    "    # gradient_accumulation_step=1,\n",
    "    \n",
    "    init_random_seed=0,\n",
    "    device=\"cuda:2\",\n",
    "    # enable_auto_mixed_precision=True,\n",
    "    \n",
    "    # log_tool=\"tensorboard\",\n",
    "    # log_dir=\"../tb_log\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 数据集\n",
    "\n",
    "数据集为通用的 torch.utils.data.Dataset 类，但在使用本训练框架时，必须为数据集类实现 `__len__` 方法，否则模型将无法定义可复现的数据加载器。\n",
    "\n",
    "在本例子中，我们定义如下的线性数据集 `LinearData`，其数据保存在 `data.pt` 文件中，为随机生成的正太随机数所给出的线性模型（截距为 0）:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import Dataset \n",
    "\n",
    "class LinearData(Dataset):\n",
    "    def __init__(self, split=\"train\") -> None:\n",
    "        super().__init__()\n",
    "        data = torch.load(\"data.pt\")[split]\n",
    "        self.x = data[\"x\"]\n",
    "        self.y = data[\"y\"]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "dataset = LinearData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 神经网络模型（torch.nn.Module）\n",
    "\n",
    "神经网络模型为通用的 `torch.nn.Module` 类。\n",
    "\n",
    "在本例子中，我们使用一个简单的线性层:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "network = nn.Linear(10, 1, bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 损失函数计算\n",
    "\n",
    "损失函数通过元组的形式传入训练器的`train`函数，其打包格式为：\n",
    "\n",
    "- ([name, ...], [loss_fn, ...])\n",
    "\n",
    "或\n",
    "\n",
    "- ([name, ...], [loss_fn, ...], [loss_weight, ...])\n",
    "\n",
    "在使用第一种打包格式时，各 loss 的计算权重默认为 1.0，损失函数的名称、计算函数、计算权重为一一对应关系。其中，`loss_fn` 接收的输入同一为两个位置参数：network 和 batch，分别对应神经网络和批数据。\n",
    "\n",
    "在本例子中，我们使用 MSE 作为损失函数:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = nn.MSELoss()\n",
    "\n",
    "def mse_loss_fn(network, batch):\n",
    "    x, y = batch \n",
    "    y_hat = network(x).squeeze()\n",
    "    return mse(y, y_hat)\n",
    "\n",
    "losses = (\n",
    "    [\"mse\"],\n",
    "    [mse_loss_fn]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 优化器部署函数（Optional）\n",
    "\n",
    "在每次训练中，训练器将部署一个新的优化器实例，实例化优化器由 `optim_fn` 这一参数实现。`optim_fn` 参数为一个函数，它接收一个神经网络 `network` 作为唯一输入，并返回一个优化器实例。在框架中，默认的 `optim_fn` 会返回一个默认的 `AdamW` 优化器。\n",
    "\n",
    "在本例子中，我们新定义一个优化器部署函数，它将在训练中使用 SGD 优化器，并使用 0.2 的学习率："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD \n",
    "\n",
    "def sgd_optim_fn(network):\n",
    "    return SGD(network.parameters(), lr=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 通过插件配置训练器功能\n",
    "\n",
    "使用 `core.plugin` 中的插件，为训练器添加功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PLUGIN](ReinitNetworkWeights) Re-initialize network weights randomly.\n",
      "[PLUGIN](EpochSave) Saving trainer to '../checkpoints/epoch-5/trainer_state_dict.pth'\n",
      "[PLUGIN](EpochSave) Saving network to '../checkpoints/epoch-5/network_state_dict.pth'\n",
      "[PLUGIN](EpochSave) Saving optimizer to '../checkpoints/epoch-5/optimizer_state_dict.pth'\n",
      "[PLUGIN](EpochSave) Saving scaler to '../checkpoints/epoch-5/scaler_state_dict.pth'\n",
      "[PLUGIN](EpochSave) Saving trainer to '../checkpoints/epoch-10/trainer_state_dict.pth'\n",
      "[PLUGIN](EpochSave) Saving network to '../checkpoints/epoch-10/network_state_dict.pth'\n",
      "[PLUGIN](EpochSave) Saving optimizer to '../checkpoints/epoch-10/optimizer_state_dict.pth'\n",
      "[PLUGIN](EpochSave) Saving scaler to '../checkpoints/epoch-10/scaler_state_dict.pth'\n",
      "[PLUGIN](EpochSave) Saving trainer to '../checkpoints/epoch-15/trainer_state_dict.pth'\n",
      "[PLUGIN](EpochSave) Saving network to '../checkpoints/epoch-15/network_state_dict.pth'\n",
      "[PLUGIN](EpochSave) Saving optimizer to '../checkpoints/epoch-15/optimizer_state_dict.pth'\n",
      "[PLUGIN](EpochSave) Saving scaler to '../checkpoints/epoch-15/scaler_state_dict.pth'\n",
      "[PLUGIN](EpochSave) Saving trainer to '../checkpoints/epoch-20/trainer_state_dict.pth'\n",
      "[PLUGIN](EpochSave) Saving network to '../checkpoints/epoch-20/network_state_dict.pth'\n",
      "[PLUGIN](EpochSave) Saving optimizer to '../checkpoints/epoch-20/optimizer_state_dict.pth'\n",
      "[PLUGIN](EpochSave) Saving scaler to '../checkpoints/epoch-20/scaler_state_dict.pth'\n"
     ]
    }
   ],
   "source": [
    "from core.plugin import EpochSavePlugin, LossLoggerPlugin\n",
    "\n",
    "trainer.extend_plugins([\n",
    "    EpochSavePlugin(\"../checkpoints\", 5),\n",
    "    LossLoggerPlugin(1)\n",
    "])\n",
    "\n",
    "trainer.train(\n",
    "    dataset=dataset,\n",
    "    network=network,\n",
    "    losses=losses,\n",
    "    optim_fn=sgd_optim_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
